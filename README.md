ğŸ“Š Real-Time Sales Data Pipeline Using Kafka, PostgreSQL & Python
(Complete End-to-End Streaming + ETL + Dashboard Project)
ğŸ“Œ Project Overview

This project demonstrates a real-time data engineering pipeline built using industry-standard tools.

It continuously generates sales events, streams them via Kafka, consumes them, stores them in PostgreSQL, and updates a dashboard every 30 seconds automatically.

ğŸš€ What this project demonstrates

âœ” Real-time data streaming
âœ” Live ingestion and processing
âœ” ETL pipeline (Extract â†’ Transform â†’ Load)
âœ” Database storage
âœ” Automated dashboards that refresh every 30 seconds
âœ” Scalable architecture used by companies like Netflix, Uber, Amazon, Flipkart, Swiggy, BigBasket, Paytm, Zomato

ğŸ¯ Pipeline Components
Layer	Technology	Purpose
Real-Time Streaming	Apache Kafka	Moves live data through pipeline
Data Generator	Python (Faker)	Creates random sales events every second
Processing Layer	Python Kafka Consumer	Reads messages & inserts into DB
Storage Layer	PostgreSQL	Stores structured sales records
Analytics Layer	Pandas + Matplotlib	Generates insights and graphs
Auto Refresh UI	Streamlit	Refreshes dashboard every 30 seconds
âš™ï¸ Architecture Diagram
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Python Producer â”‚ ----> â”‚   Kafka Topic: sales   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Python Consumer  â”‚ ----> â”‚   PostgreSQL     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Streamlit Dashboard (Auto refresh 30 sec)    â”‚
           â”‚  Pandas + Matplotlib Visualizations           â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§© Technologies Used
Component	Technology
Real-Time Streaming	Apache Kafka
Data Generation	Python (Faker)
Data Ingestion	Python Kafka Consumer
Storage	PostgreSQL
Visualization	Pandas, Matplotlib, Streamlit
Programming Language	Python 3.13
ğŸ“‚ Project Structure
sales_kafka_project/
â”‚â”€â”€ sales_producer.py       â†’ Generates live sales stream
â”‚â”€â”€ sales_consumer.py       â†’ Consumes Kafka data & stores in DB
â”‚â”€â”€ dashboard.py            â†’ Auto-updating analytics dashboard
â”‚â”€â”€ sales.csv               â†’ Sample dataset for offline demo
â”‚â”€â”€ requirements.txt        â†’ Python dependencies
â”‚â”€â”€ README.md               â†’ Project documentation

ğŸ”¥ 1. Python Producer â€” Real-Time Data Generator

sales_producer.py generates new sales every second:

product_name

quantity

price

timestamp

Then sends each event to:

Kafka Topic â†’ sales-topic

Output looks like:

Sent: {'product_name': 'Mobile', 'quantity': 3, 'price': 23499, ...}

ğŸ”„ 2. Python Consumer â€” Ingestion Layer

sales_consumer.py listens to sales-topic and inserts data into PostgreSQL table.

PostgreSQL Table
CREATE TABLE sales_data (
    sale_id SERIAL PRIMARY KEY,
    product_name VARCHAR(50),
    quantity INT,
    price NUMERIC(10,2),
    timestamp TIMESTAMP
);


Consumer output:

Inserted: {'product_name': 'Laptop', 'quantity': 2, ...}

ğŸ—„ï¸ 3. PostgreSQL â€” Storage Layer

Data is stored in:

Database: salesdb

Table: sales_data

This forms the warehouse layer for analysis.

ğŸ“Š 4. Real-Time Dashboard (Auto Refresh Every 30 Seconds)

dashboard.py shows:

ğŸ“ˆ Key Charts

Daily Revenue Trend

Top 10 Products by Revenue

Quantity Sold Per Day

Price Distribution Curve

KPIs (Total Sales, Revenue, Average Price, Top Product)

ğŸ” Auto-Refresh (every 30 seconds)

The dashboard automatically fetches the latest DB records every 30 seconds:

st_autorefresh(interval=30 * 1000, key="auto_refresh")


That means:

Even if producer is running and generating 1000s of new rows

Even if consumer is inserting them live

The dashboard keeps updating automatically WITHOUT clicking refresh

This impresses teachers a lot because it shows true real-time BI.

ğŸ›  How to Run the Entire Project (Step-by-Step)
âœ” STEP 1 â€” Start Zookeeper
cd ~/Kafka
bin/zookeeper-server-start.sh config/zookeeper.properties

âœ” STEP 2 â€” Start Kafka Broker
cd ~/Kafka
bin/kafka-server-start.sh config/server.properties

âœ” STEP 3 â€” Create Kafka Topic
bin/kafka-topics.sh --create --topic sales-topic --bootstrap-server localhost:9092

âœ” STEP 4 â€” Run Producer (Live Data)
cd ~/sales_kafka_project
python3 sales_producer.py


You will see new data generated EVERY SECOND.

âœ” STEP 5 â€” Run Consumer
python3 sales_consumer.py


New rows start populating PostgreSQL LIVE.

âœ” STEP 6 â€” Run Analytics Dashboard
streamlit run dashboard.py


You will see:

real-time charts

updated KPIs

new rows every 30 seconds

growing revenue curves

ğŸ”„ Complete Data Flow (Simple Explanation)

Python Producer
â†’ Creates fake sales every second

Kafka Topic
â†’ Acts as a real-time buffer

Kafka Consumer
â†’ Reads stream continuously

PostgreSQL
â†’ Stores clean structured data

Streamlit Dashboard
â†’ Auto-refreshes every 30 seconds
â†’ Shows latest graphs & KPIs
