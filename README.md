ğŸ“Š Real-Time Sales Data Pipeline using Kafka, PostgreSQL & Python

This project demonstrates a real-time data engineering pipeline using industry-standard tools.

It continuously generates sales events, streams them via Kafka, consumes them, stores them in PostgreSQL, and updates a real-time dashboard every 30 seconds automatically.

ğŸš€ What this project demonstrates

âœ” Real-time data streaming
âœ” Live ingestion + processing
âœ” ETL pipeline (Extract â†’ Transform â†’ Load)
âœ” Database storage
âœ” Automated dashboards (refresh every 30 sec)
âœ” Scalable system used by companies like Netflix, Uber, Amazon, Flipkart, Swiggy, Paytm

ğŸ§© Pipeline Components
Layer	Technology	Purpose
Real-Time Streaming	Apache Kafka	Moves live data
Data Generator	Python (Faker)	Creates new sales every second
Processing Layer	Python Kafka Consumer	Reads Kafka messages & inserts into DB
Storage Layer	PostgreSQL	Stores structured sales data
Analytics Layer	Pandas + Matplotlib	Generates insights & graphs
UI Layer	Streamlit	Auto-refresh dashboard (every 30 sec)
ğŸ”¥ Architecture Diagram
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Producer   â”‚ ----> â”‚   Kafka Topic: sales     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Consumer   â”‚ ----> â”‚      PostgreSQL          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Streamlit Dashboard (Auto-refresh every 30 sec)   â”‚
        â”‚ Pandas + Matplotlib Visualizations                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ›  Technologies Used
Component	Technology
Streaming	Apache Kafka
Data Generation	Python + Faker
Consumer	Python Kafka Client
Database	PostgreSQL
Visualizations	Pandas, Matplotlib, Streamlit
Language	Python 3.13
ğŸ“ Project Structure
sales_kafka_project/
â”‚â”€â”€ sales_producer.py        # Real-time data generator
â”‚â”€â”€ sales_consumer.py        # Kafka â†’ PostgreSQL consumer
â”‚â”€â”€ dashboard.py             # Auto-updating real-time dashboard
â”‚â”€â”€ sales.csv                # Sample dataset (optional)
â”‚â”€â”€ requirements.txt         # Dependencies
â”‚â”€â”€ README.md                # Documentation

ğŸ”¥ 1. Python Producer â€” Real-Time Data Generator

sales_producer.py generates new sales every second:

product_name

quantity

price

timestamp

Sends each as JSON to:

â¡ï¸ Kafka Topic: sales-topic

Example:

Sent: {"product_name": "Mobile", "quantity": 3, "price": 24999, ...}

ğŸ”„ 2. Python Consumer â€” Ingestion Layer

sales_consumer.py listens to Kafka and inserts records into PostgreSQL table:

CREATE TABLE sales_data (
    sale_id SERIAL PRIMARY KEY,
    product_name VARCHAR(50),
    quantity INT,
    price NUMERIC(10,2),
    timestamp TIMESTAMP
);


Example:

Inserted: {"product_name": "Laptop", "quantity": 2, ...}

ğŸ—„ 3. PostgreSQL Storage

Data stored in:

Database: salesdb

Table: sales_data

This stores clean structured data for analysis.

ğŸ“Š 4. Real-Time Analytics Dashboard

Auto-refresh every 30 seconds

dashboard.py displays:

Total revenue

Total sales

Top products

Revenue over time

Quantity trends

Price distribution

Auto-refresh code:
st_autorefresh(interval=30 * 1000, key="refresh")


âœ” Fetches NEW rows from PostgreSQL
âœ” Updates all charts & KPIs automatically
âœ” No need to reload manually

ğŸ›  How to Run the Project
âœ” Step 1 â€” Start Zookeeper
cd ~/Kafka
bin/zookeeper-server-start.sh config/zookeeper.properties

âœ” Step 2 â€” Start Kafka Server
bin/kafka-server-start.sh config/server.properties

âœ” Step 3 â€” Create Kafka Topic
bin/kafka-topics.sh --create --topic sales-topic --bootstrap-server localhost:9092

âœ” Step 4 â€” Start Producer
python3 sales_producer.py

âœ” Step 5 â€” Start Consumer
python3 sales_consumer.py

âœ” Step 6 â€” Start Dashboard
streamlit run dashboard.py

ğŸ” Complete Flow (Simple Explanation)
Python Producer â†’ Kafka â†’ Python Consumer â†’ PostgreSQL â†’ Streamlit Dashboard


Producer: Creates fake real-time sales

Kafka: Streams the data

Consumer: Inserts into PostgreSQL

DB: Stores all sales

Dashboard: Auto-refreshes every 30 sec to show new data
